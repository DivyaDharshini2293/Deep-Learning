{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZKPnWlexksQMsrWrO7bcJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DivyaDharshini2293/Deep-Learning/blob/main/BYTS_Pytorch_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxBRX7Z8nPLn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.1307,),(0.3081,))])"
      ],
      "metadata": {
        "id": "q4_xRqNOntW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=datasets.MNIST('data',train=True,download=True,transform=transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB2uoN_eqEWf",
        "outputId": "0be0def8-c139-4284-cb20-e23818abe33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=64,shuffle=True)"
      ],
      "metadata": {
        "id": "g15FrnYPqTd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Neural,self).__init__()\n",
        "    self.fc1=nn.Linear(28*28,128)# 28*28 is to flatten the input, 128 is the output size\n",
        "    self.fc2=nn.Linear(128,64) #Output of fc1(128) is input of fc2 and its output is 10(MNIST-10 digits)\n",
        "    self.fc3=nn.Linear(64,32)\n",
        "    self.fc4=nn.Linear(32,16)\n",
        "    self.fc5=nn.Linear(16,10)\n",
        "  def forward(self,x):\n",
        "    x=x.view(-1,28*28)\n",
        "    x=torch.relu(self.fc1(x))# activation function of hidden layer\n",
        "    x=torch.relu(self.fc2(x))\n",
        "    x=torch.relu(self.fc3(x))\n",
        "    x=torch.relu(self.fc4(x))\n",
        "    x=self.fc5(x) # output layer\n",
        "    return x\n",
        "net=Neural() #Intanciation of class using object."
      ],
      "metadata": {
        "id": "hXybi4OoqoWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.CrossEntropyLoss() #Definig the loss function\n",
        "optimizer=optim.SGD(net.parameters(),lr=0.01,momentum=0.5)"
      ],
      "metadata": {
        "id": "k5EtolDx1eZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=10\n",
        "for epoch in range(num_epochs):\n",
        "  for batch_idx,(data,target) in enumerate (train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output=net(data)\n",
        "    loss =criterion(output,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 100==0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sO5Tqoug1qhR",
        "outputId": "728a0fa0-9b83-4d58-ddc5-fdcfb77fdc11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.305350\n",
            "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 2.298857\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 2.282757\n",
            "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 2.268291\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 2.199246\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 1.957878\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.249607\n",
            "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.821243\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.539853\n",
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.600830\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.648671\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.590843\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.401783\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.358959\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.507462\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.472520\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.283936\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.356515\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.371836\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.201653\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.186195\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.569757\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.216629\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.142027\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.367949\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.095338\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.190920\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.096059\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.276799\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.073497\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.392868\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.095184\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.042792\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.389119\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.261043\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.110867\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.165916\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.244781\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.278852\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.192684\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.300540\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.174430\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.170595\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.057859\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.058200\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.199212\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.064117\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.092352\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.129688\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.118447\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.075285\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.176438\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.098028\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.167523\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.077404\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.092306\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.033792\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.081684\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.051623\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.081578\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.046357\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.017203\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.097742\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.123580\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.020349\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.100692\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.150342\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.074028\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.127408\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.150765\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.025830\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.044095\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.043140\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.043792\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.007999\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.054855\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.014033\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.168099\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.027485\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.057669\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.072215\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.012179\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.019684\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.012694\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.018066\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.023458\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.029019\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.035040\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.048943\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.005383\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.112063\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.194003\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.015247\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.123362\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.099246\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.020276\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.084492\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.050919\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.097419\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.016265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset=datasets.MNIST('data',train=False,download=True,transform=transform)\n",
        "test_loader=torch.utils.data.DataLoader(test_dataset,batch_size=1000,shuffle=True)\n",
        "correct=0\n",
        "total=0\n",
        "with torch.no_grad():\n",
        "  output=net(data)\n",
        "  _,predicted=torch.max(output.data,1)\n",
        "  total+=target.size(0)\n",
        "  correct+=(predicted==target).sum().item()"
      ],
      "metadata": {
        "id": "1PT4FsF311eJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKvgtXAq2kse",
        "outputId": "2c0af8f1-6e77-4b2b-bd1a-9cae9c2a65de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 100 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations for the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "train_dataset = CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the neural network architecture\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 512)  # 3 channels, 32x32 image size\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 100)  # 100 output classes for CIFAR-100\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3 * 32 * 32)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the neural network\n",
        "model = SimpleNN()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Using Adam optimizer\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:  # Print every 200 mini-batches\n",
        "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
        "                  f'Step [{i + 1}/{len(train_loader)}], '\n",
        "                  f'Loss: {running_loss / 200:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Finished Training\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Accuracy on the test set: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4B2NFByAhId",
        "outputId": "bb7c32b5-fdc7-40f7-94cf-a99ad2d2282d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/10], Step [200/782], Loss: 4.0552\n",
            "Epoch [1/10], Step [400/782], Loss: 3.7218\n",
            "Epoch [1/10], Step [600/782], Loss: 3.6194\n",
            "Epoch [2/10], Step [200/782], Loss: 3.3689\n",
            "Epoch [2/10], Step [400/782], Loss: 3.3669\n",
            "Epoch [2/10], Step [600/782], Loss: 3.3365\n",
            "Epoch [3/10], Step [200/782], Loss: 3.1422\n",
            "Epoch [3/10], Step [400/782], Loss: 3.1475\n",
            "Epoch [3/10], Step [600/782], Loss: 3.1718\n",
            "Epoch [4/10], Step [200/782], Loss: 2.9362\n",
            "Epoch [4/10], Step [400/782], Loss: 3.0073\n",
            "Epoch [4/10], Step [600/782], Loss: 3.0074\n",
            "Epoch [5/10], Step [200/782], Loss: 2.8138\n",
            "Epoch [5/10], Step [400/782], Loss: 2.8300\n",
            "Epoch [5/10], Step [600/782], Loss: 2.8893\n",
            "Epoch [6/10], Step [200/782], Loss: 2.6500\n",
            "Epoch [6/10], Step [400/782], Loss: 2.7266\n",
            "Epoch [6/10], Step [600/782], Loss: 2.7380\n",
            "Epoch [7/10], Step [200/782], Loss: 2.5202\n",
            "Epoch [7/10], Step [400/782], Loss: 2.6037\n",
            "Epoch [7/10], Step [600/782], Loss: 2.6198\n",
            "Epoch [8/10], Step [200/782], Loss: 2.3956\n",
            "Epoch [8/10], Step [400/782], Loss: 2.4513\n",
            "Epoch [8/10], Step [600/782], Loss: 2.5321\n",
            "Epoch [9/10], Step [200/782], Loss: 2.2666\n",
            "Epoch [9/10], Step [400/782], Loss: 2.3251\n",
            "Epoch [9/10], Step [600/782], Loss: 2.3925\n",
            "Epoch [10/10], Step [200/782], Loss: 2.1354\n",
            "Epoch [10/10], Step [400/782], Loss: 2.2270\n",
            "Epoch [10/10], Step [600/782], Loss: 2.2875\n",
            "Finished Training\n",
            "Accuracy on the test set: 23.41%\n"
          ]
        }
      ]
    }
  ]
}